<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="toggle.css" type="text/css" />
<script src="toggle.js" type="text/javascript"></script>
<title>Program for the working group on low-rank approximation</title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Program for the working group on low-rank approximation</h1>
<div id="subtitle">within the <a href="http://events.math.unipd.it/drwa13/">Dolomites Research Week on Approximation,</a> Alba di Canazei, Italy, September 8&ndash;13, 2013.</div>
</div>
<h2>Monday </h2>
<ul>
<li><p>11:00-12:00: <a href="http://www.ece.neu.edu/ece/index.php/component/content/123?task=view">Mario Sznaier</a></p>
</li>
<li><p>12:00-12:30: <a href="http://www.montefiore.ulg.ac.be/~mishra/">Bamdev Mishra</a> <i>Manopt: A Matlab toolbox for optimization on manifolds</i> <a href="http://www.manopt.org">[link]</a></p>
</li>
<li><p>17:00-17:45: <a href="http://homepages.vub.ac.be/~kusevich/">Konstantin Usevich</a> <i>Adjusted least squares estimator for algebraic hypersurface fitting</i><br />
<a href="javascript:toggleAbs('kostia')">[abstract]</a>  <br/><span id="bib_kostia" class="noshow">The problem of algebraic hypersurface fitting is considered in an estimation framework: given a set of noisy data points, estimate the coefficients of a corresponding implicit multivariate polynomial equation. The adjusted least squares estimator proposed accounts for the bias present in the ordinary least squares estimator. An algorithm for computing the correction for an arbitrary support of the polynomial equation is presented and applied to the related problem of subspace clustering. </span> </p>
</li>
<li><p>17:45-18:30: <a href="http://homepages.vub.ac.be/~imarkovs/homepage.html">Ivan Markovsky</a><br /> <i>Low-rank approximation problems in system identification</i>
<a href="javascript:toggleAbs('ivan')">[abstract]</a>  <br/><span id="bib_ivan" class="noshow">The notion of linear time-invariant model complexity is quantified and related to the rank of a mosaic Hankel matrix constructed from a set of trajectories of the system. This fact makes mosaic Hankel structured low-rank approximation (SLRA) a powerful tool for linear time-invariant system identification. It was shown in the past that errors-in-variables and output error identification problem with known noise covariance structure can be solved by generic SLRA methods. The SLRA approach has attractive numerical properties and can deal with exact and missing data values. Finally, we present work in progress on solving auto-regressive moving-average exogenous identification problems by SLRA. </span> </p>
</li>
</ul>
<h2>Tuesday</h2>
<ul>
<li><p>11:00-12:00: <a href="http://homes.esat.kuleuven.be/~delathau/">Lieven De Lathauwer</a>,<br /> <i>Numerical approximation of higher-order tensors: An introduction</i>
<a href="javascript:toggleAbs('lieven')">[abstract]</a>  <br/><span id="bib_lieven" class="noshow">We give an overview of basic tensor decompositions and explain their general use. We pay special attention to the use of tensor decompositions for approximation purposes. We discuss numerical strategies for tensor approximation. Important progress has recently been made. It has been recognized that tensor product structure allows a very efficient storage and handling of the Jacobian and (approximate) Hessian of the cost function. On the other hand, multilinearity allows global optimization in (scaled) line and plane search. Although there are many possibilities for decomposition symmetry and factor structure, these may be conveniently handled. We demonstrate the algorithms using Tensorlab, our recently published MATLAB toolbox for tensors and tensor computations. (Joint work with Laurent Sorber and Marc Van Barel)  </span> </p>
</li>
<li><p>12:00-12:30: <a href="http://homepages.vub.ac.be/~mishteva/">Mariya Ishteva</a><br /> <i>Low multilinear rank approximation of tensors by structured matrix low-rank approximation</i>
<a href="javascript:toggleAbs('maria')">[abstract]</a>  <br/><span id="bib_maria" class="noshow">  We first consider symmetric higher-order tensors and discuss a novel way to perform symmetric low multilinear rank approximation. Instead of representing the approximation as a product of factors with reduced dimensions, we make use of the fact that a matrix is rank deficient if it has nonzero null space (nonzero kernel). We show how to reformulate the tensor approximation problem as structured matrix low-rank approximation. In addition, by imposing a constraint on the kernel matrix of the approximation, our approach is applicable to general (non-symmetric) tensors. Finally, we deal with affinely structured tensors and obtain (locally) best low multilinear approximation with the same structure. </span></p>
</li>
<li><p>17:00-17:45: <a href="https://sites.google.com/site/nicolasgillis/">Nicolas Gillis</a><br /> <i>Recent advances in nonnegative matrix factorization</i>
<a href="javascript:toggleAbs('nicolas')">[abstract]</a>  <br/><span id="bib_nicolas" class="noshow">  Nonnegative matrix factorization (NMF) is a powerful dimensionality reduction technique as it automatically extracts sparse and meaningful features from a set of nonnegative data vectors. NMF has many applications; for example in text mining, graph clustering, and hyperspectral imaging. Unfortunately, NMF is NP-hard in general, and highly ill-posed. However, NMF has been shown recently to be tractable under the separability assumption, which amounts for the columns of the input data matrix to belong to the convex cone generated by a small number of columns. Since then, several algorithms have been proposed to handle this subclass of NMF problems under any small perturbation of the input matrix (these are called near-separable NMF problems). In this talk, we present some recent advances for solving near-separable NMF, including an approach using successive orthogonal projections (joint work with S. Vavasis), and another using linear optimization (joint work with R. Luce).</span></p>
</li>
<li><p>17:45-18:30: <a href="http://www.di.ens.fr/~lefevrea/">Augustin Lefèvre</a></p>
</li>
</ul>
<h2>Thursday</h2>
<ul>
<li><p>16:30-17:10: <a href="http://users.ba.cnr.it/iac/irmanm21/">Nicola Mastronardi</a></p>
</li>
<li><p>17:20-18:00: <a href="http://homes.esat.kuleuven.be/~smc/person.php?persid=299">Marco Signoretto</a><br /> <i>Tensor estimation in reproducing kernel Hilbert spaces with applications</i> 
<a href="javascript:toggleAbs('marco')">[abstract]</a>  <br/><span id="bib_marco" class="noshow">Recently there has been an increasing interest in the cross-fertilization of ideas coming from (convex) optimization, kernel methods and tensor-based techniques. In this talk we discuss the estimation of tensors from data in the unifying framework of reproducing kernel Hilbert spaces (RKHSs). The methodology is based on a multilinear generalization of spectral penalties and relies on a novel representer theorem. When the RKHS is defined on the Cartesian product of finite sets, our problem formulation specializes into existing matrix and tensor completion problems. Additionally, the approach leads to the extension of kernel-based frameworks based on operator estimation. This includes multi-task learning, collaborative filtering and regularization networks. We illustrate applications and highlight the advantages with respect to the existing learning techniques.</span></p>
</li>
<li><p>18:00-18:30: <a href="http://www.montefiore.ulg.ac.be/~mishra/">Bamdev Mishra</a><br /> <i>Fixed-rank optimization on Riemannian quotient manifolds</i> 
<a href="javascript:toggleAbs('bamdev')">[abstract]</a>  <br/><span id="bib_bamdev" class="noshow">Recently, the problem of low-rank matrix completion has attracted a lot of attention. Consequently, a lot of algorithms about fixed-rank optimization has been proposed. Motivated by this, we are interested in three popular fixed-rank factorizations that have been used extensively in the optimization community.<br/>The search space of all the three fixed-rank factorizations will be shown to admit a quotient structure, in particular, it will be given a more general Riemannian quotient structure. This structure embodies a product of well-known manifolds and the Riemannian framework will allow us exploit this and to systematically develop the tools necessary for optimization. As we move on, two important concepts of symmetry and scaling will be emphasized. A short discussion on the specific choice of the Riemannian metric and the tradeoff between geometry and numerics will be presented.<br/>The resulting class of algorithms, while maintaining complete generality, compete effectively with other state-of-the-art algorithms for the low-rank matrix completion problem on a number of problem instances.<br/>I will also present the new optimization toolbox Manopt that has been specifically designed to work with optimization on manifolds. The toolbox provides a unique platform to experiment with various manifolds and their geometries and comes bundled with a number of optimization schemes. Optimizing a cost function on a specific manifold can be implemented almost painlessly.<br/>Finally, I will present few open questions and give a future outlook of research in this direction.</span></p>
</li>
</ul>
<h2>Friday</h2>
<ul>
<li><p>11:00-11:30: <a href="https://www.aut.bme.hu/Staff/kolixx">Kolumbán Sándor</a> <i>Decreasing complexity of SDP relaxations for polynomial optimization by forcing low-rank constraints</i> 
<a href="javascript:toggleAbs('kol')">[abstract]</a>  <br/><span id="bib_kol" class="noshow">The size of SDP relaxations corresponding to polynomial optimization problems increase combinatorially with the degree of the relaxation. Forcing low rank constraints on the moment matrices involved in the LMI constraints opens the possibility to reduce the size of the involved SDP. This may lead to better computational performance for other relaxations.</span></p>
</li>
<li><p>11:30-12:15: <a href="http://people.tuebingen.mpg.de/fdinuzzo/">Francesco Dinuzzo</a> <i>Low Rank Output Kernels for Multi-Task Learning Problems</i> 
<a href="javascript:toggleAbs('francesco')">[abstract]</a>  <br/><span id="bib_francesco" class="noshow">Low Rank Output Kernel Learning (LROKL) is a kernel-based nonlinear generalization of Reduced Rank Regression that allows to simultaneously solve multiple estimation tasks and reveal a shared low-dimensional subspace capturing inter-task relationships. This talk will discuss several alternative formulations of LROKL, numerical strategies to train the associated model, and applications to a variety of multi-task learning problems.</span></p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2013-07-25 12:22:53 CEST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</div>
</body>
</html>
