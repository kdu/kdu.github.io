<h1>t.bib</h1><a name="my-nucnrm"></a><pre>
@techreport{<a href="t.html#my-nucnrm">my-nucnrm</a>,
  author = {I. Markovsky},
  title = {Data modeling using the nuclear norm heuristic},
  institution = {ECS, Univ. of Southampton},
  year = {2011},
  number = {21936},
  url = {<a href="http://eprints.ecs.soton.ac.uk/21936/">http://eprints.ecs.soton.ac.uk/21936/</a>}
}
</pre>

<a name="overview"></a><pre>
@techreport{<a href="t.html#overview">overview</a>,
  author = {I. Markovsky},
  title = {Recent progress on structured low-rank approximation},
  institution = {Vrije Univ. Brussel},
  year = {2013},
  note = {Submitted on 16/05/2013 to {\em Signal Processing}},
  url = {<a href="http://homepages.vub.ac.be/~imarkovs/recent-publications.html">http://homepages.vub.ac.be/~imarkovs/recent-publications.html</a>},
  pdf = {<a href="http://homepages.vub.ac.be/~imarkovs/publications/overview.pdf">http://homepages.vub.ac.be/~imarkovs/publications/overview.pdf</a>},
  abstract = {Rank deficiency of a data matrix is equivalent to the existence of an exact linear model for the data. For the purpose of linear static modeling, the matrix is unstructured and the corresponding modeling problem is an approximation of the matrix by another matrix of a lower rank. In the context of linear time-invariant dynamic models, the appropriate data matrix is Hankel and the corresponding modeling problems becomes structured low-rank approximation. Low-rank approximation has applications in: system identification; signal processing, machine learning, and computer algebra, where different types of structure and constraints occur. This paper gives an overview of recent progress in efficient local optimization algorithms for solving weighted mosaic-Hankel structured low-rank approximation problems. In addition, the data matrix may have missing elements and elements may be specified as exact. The described algorithms are implemented in a publicly available software package. Their application to system identification, approximate common divisor, and data-driven simulation problems is described in this paper and is illustrated by reproducible simulation examples. As a data modeling paradigm the low-rank approximation setting is closely related to the the behavioral approach in systems and control, total least squares, errors-in-variables modeling, principal component analysis, and rank minimization.}
}
</pre>

<a name="pltv"></a><pre>
@techreport{<a href="t.html#pltv">pltv</a>,
  author = {I. Markovsky and J. Goos and K. Usevich and R. Pintelon},
  title = {Subspace identification of autonomous linear periodically time-varying systems},
  institution = {Vrije Univ. Brussel},
  note = {Submitted on 03/2013 to {\em Automatica}.},
  year = {2013},
  pdf = {<a href="http://homepages.vub.ac.be/~imarkovs/publications/pltv-rev.pdf">http://homepages.vub.ac.be/~imarkovs/publications/pltv-rev.pdf</a>},
  abstract = {Subsampling of a linear periodically time-varying system results in a collection of linear time-invariant systems with common poles. This key fact, known as ``lifting'', is used in a two step realization method. The first step is the realization of the time-invariant dynamics (the lifted system). Computationally, this step is a rank-revealing factorization of a block-Hankel matrix. The second step derives a state space representation of the periodic time-varying system. It is shown that no extra computations are required in the second step. The computational complexity of the overall method is therefore equal to the complexity for the realization of the lifted system. A modification of the realization method is proposed, which makes the complexity independent of the parameter variation period. Replacing the rank-revealing factorization in the realization algorithm by structured low-rank approximation yields a maximum likelihood identification method. Existing methods for structured low-rank approximation are used to identify efficiently linear periodically time-varying system.}
}
</pre>

<a name="armax"></a><pre>
@techreport{<a href="t.html#armax">armax</a>,
  author = {I. Markovsky and K. Usevich},
  title = {{ARMAX} identification by structured low-rank approximation},
  institution = {Vrije Univ. Brussel},
  year = {2013}
}
</pre>

<a name="slra-agcd"></a><pre>
@techreport{<a href="t.html#slra-agcd">slra-agcd</a>,
  author = {K. Usevich and I. Markovsky},
  title = {Variable projection methods for approximate (greatest) common divisor computations},
  year = {2013},
  note = {Submitted on 25/04/2013 to {\em Journal of Symbolic Computation}.},
  institution = {Vrije Univ. Brussel},
  url = {<a href="http://arxiv.org/abs/1304.6962">http://arxiv.org/abs/1304.6962</a>},
  pdf = {<a href="http://arxiv.org/pdf/1304.6962v1">http://arxiv.org/pdf/1304.6962v1</a>},
  abstract = {We consider the problem of finding for a given $N$-tuple of polynomials the closest $N$-tuple that has a common divisor of degree at least $d$. Extended weighted Euclidean semi-norm of coefficients is used as a measure of closeness. Two equivalent formulations of the problem are considered: (i) direct optimization over common divisors and cofactors, and (ii) Sylvester lowrank approximation. We use the duality between least-squares and least-norm problems to show that (i) and (ii) are closely related to mosaic Hankel low-rank approximation. This allows us to apply recent results on complexity and accuracy of computations for mosaic Hankel low-rank approximation. We develop optimization methods based on the variable projection principle. These methods have linear complexity in the degrees of the polynomials if either $d$ is small or $d$ is of the same order as the degrees of the polynomials. We provide a software implementation that is based on a software package for structured low-rank approximation.}
}
</pre>

<a name="UM12autart"></a><pre>
@techreport{<a href="t.html#UM12autart">UM12autart</a>,
  author = {K. Usevich and I. Markovsky},
  title = {Optimization on a {G}rassmann manifold with application to system identification},
  institution = {Vrije Univ. Brussel},
  note = {Submitted on 03/2013 to {\em Automatica}.},
  year = {2012},
  url = {<a href="http://homepages.vub.ac.be/~kusevich/preprints.html">http://homepages.vub.ac.be/~kusevich/preprints.html</a>},
  pdf = {<a href="http://homepages.vub.ac.be/~kusevich/preprints/usevich_markovsky_aut2012.pdf">http://homepages.vub.ac.be/~kusevich/preprints/usevich_markovsky_aut2012.pdf</a>},
  abstract = {In this paper, we consider the problem of optimization of a cost function on a Grassmann manifold. This problem appears in system identification in the behavioral setting, which is a structured low-rank approximation problem. We develop a new method for local optimization on the Grassmann manifold with switching coordinate charts. This method reduces the optimization problem on the manifold to an optimization problem in a bounded domain of an Euclidean space. Our experiments show that this method is competitive with state- of-the-art retraction-based methods. Compared to retraction-based methods, the proposed method allows to incorporate easily an arbitrary optimization method for solving the optimization subproblem in the Euclidean space.},
  keywords = {system identification, over-parameterized models, Grassmann manifold, coordinate charts, structured low-rank approximation, optimization}
}
</pre>

<a name="rslra"></a><pre>
@techreport{<a href="t.html#rslra">rslra</a>,
  author = {M. Ishteva and K. Usevich and I. Markovsky},
  title = {Regularized structured low-rank approximation},
  year = {2013},
  institution = {Vrije Univ. Brussel},
  note = {Submitted on 02/08/2013 to {\em SIAM J. Matrix Anal. Appl.}},
  keywords = {low-rank approximation, affine structure, regularization, system identification, approximate greatest common divisor},
  abstract = {We consider the problem of approximating an affinely structured matrix, for example a Hankel matrix, by a low-rank matrix with the same structure. This problem occurs in system identification, signal processing and computer algebra, among others. We impose the low-rank by modeling the approximation as a product of two factors with reduced dimension. The structure of the low-rank model is enforced by introducing a regularization term in the objective function. The proposed local optimization algorithm is able to solve the weighted structured low-rank approximation problem, as well as to deal with the cases of missing or fixed elements. In contrast to approaches based on kernel representations (in linear algebraic sense), the proposed algorithm is designed to address the case of small targeted rank. We compare it to existing approaches on numerical examples of system identification, approximate greatest common divisor problem, and symmetric tensor decomposition and demonstrate its consistently good performance.}
}
</pre>

<a name="KanIshPar13"></a><pre>
@techreport{<a href="t.html#KanIshPar13">KanIshPar13</a>,
  author = {R. Kannan and M. Ishteva and H. Park},
  title = {Bounded Matrix Factorization for Recommender System},
  number = {},
  year = {2013},
  optnote = {submitted},
  keywords = {low rank approximation, recommender systems, bound constraints, matrix factorization, block coordinate descent method, scalable algorithm},
  abstract = {Matrix factorization has been widely utilized as a latent-factor model for solving the recommender system problem using collaborative filtering. For a recommender system, all the ratings in the rating matrix are bounded within $[r_{min},r_{max}]$. In this paper, we propose a new improved matrix factorization approach for such a rating matrix, called  Bounded Matrix Factorization (BMF) which imposes a lower and an upper bound on every estimated missing element of the rating matrix. We present an efficient algorithm to solve BMF based on the block coordinate descent method. We show that our algorithm is scalable for large matrices with missing elements on multi core systems with low memory. We present substantial experimental results illustrating that the proposed method  outperforms the state of the art algorithms for recommender system such as Stochastic Gradient Descent, Alternating Least Squares with regularization, SVD++ and Bias-SVD on real world data sets such as Jester, Movielens, Book crossing, Online dating and Netflix.}
}
</pre>

<hr><p><em>This file was generated by
<a href="http://www.lri.fr/~filliatr/bibtex2html/">bibtex2html</a> 1.97.</em></p>
